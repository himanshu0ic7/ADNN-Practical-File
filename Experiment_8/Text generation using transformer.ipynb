{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29db4023",
   "metadata": {},
   "source": [
    "<h1><center>Experiment: 8</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4274349",
   "metadata": {},
   "source": [
    "# Aim: \n",
    "To perform text generation using Transformers, leveraging their ability to model\n",
    "sequential data and generate coherent, context-aware text outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5696494",
   "metadata": {},
   "source": [
    "# Theory:\n",
    "This experiment demonstrates text generation using a transformer model, a neural network architecture renowned for its attention mechanism, which enables the model to weigh the importance of different input words during generation. The process involves tokenizing input text, setting parameters like temperature for randomness, and generating output tokens using the model. Additionally, the experiment includes visualizing attention weights across multiple heads, allowing insights into how the model interprets and focuses on various parts of the input during the text generation process. This approach showcases the transformative power of attention in improving natural language understanding and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89998ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14dbd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 09:27:04.000688: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 09:27:04.000752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 09:27:04.002304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 09:27:04.010485: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dee86d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n",
       "1  Your Brain On Coronavirus\\n\\nA guide to the cu...\n",
       "2  Mind Your Nose\\n\\nHow smell training can chang...\n",
       "3  Passionate about the synergy between science a...\n",
       "4  You’ve heard of him, haven’t you? Phineas Gage..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/workspace/ADNN/Exp-5/Dataset/medium_articles.csv\",\n",
    ")\n",
    "df = df[[\"text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482f5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(df))  # first 90% will be train, rest val\n",
    "train_examples = df[:n]\n",
    "val_examples = df[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb645473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 09:27:17.919116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17947 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "train_examples = tf.data.Dataset.from_tensor_slices((train_examples))\n",
    "val_examples = tf.data.Dataset.from_tensor_slices((val_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e7d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000  # Maximum vocab size\n",
    "BATCH_SIZE = 32\n",
    "MAX_TOKENS = 128\n",
    "BUFFER_SIZE = 1000 \n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens=max_features,\n",
    ")\n",
    "vectorize_layer.adapt(train_examples, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34cf9abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66202437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(data):\n",
    "    x = vectorize_layer(data)\n",
    "    x = x[:, :(MAX_TOKENS)]  # Trim to MAX_TOKENS\n",
    "    X_train = x[:, :-1]  # Shift by one\n",
    "    y_train = x[:, 1:]  # Shift by one\n",
    "    return (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68217de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "        ds.shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a69a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation set batches\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597fdfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 127)\n",
      "(32, 127)\n"
     ]
    }
   ],
   "source": [
    "for X_train, y_train in train_batches.take(1):\n",
    "    break\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6221d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch, y_batch in train_batches.take(1):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d131eea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 127])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape\n",
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1f1e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 512)\n"
     ]
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# Check the shape.\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap=\"RdBu\")\n",
    "plt.ylabel(\"Depth\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8925814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df674afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x, value=x, key=x, return_attention_scores=True, use_causal_mask=True\n",
    "        )\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ce9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded output shape: (32, 127, 512)\n",
      "(32, 127, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 09:29:40.939353: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "d_model = 512\n",
    "vocab_size = 8000\n",
    "# Create positional embedding layer\n",
    "pos_embedding_layer = PositionalEmbedding(vocab_size, d_model)\n",
    "x_batch_emb = pos_embedding_layer(x_batch)\n",
    "\n",
    "# print(\"Input shape:\", x_batch.shape)\n",
    "print(\"Embedded output shape:\", x_batch_emb.shape)\n",
    "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "x_batch_emb = sample_csa(x_batch_emb)\n",
    "# print(x_batch.shape)\n",
    "print(x_batch_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a16d697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(d_model),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d89d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.causal_self_attention.last_attn_scores\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4597ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(\n",
    "                d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd76b9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 127)\n",
      "(32, 127, 512)\n",
      "(32, 127, 512)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(\n",
    "    num_layers=4, d_model=512, num_heads=8, dff=2048, vocab_size=8000\n",
    ")\n",
    "\n",
    "output = sample_decoder(x=x_batch)\n",
    "\n",
    "# Print the shapes.\n",
    "print(x_batch.shape)\n",
    "print(x_batch_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e2eae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your PositionalEmbedding, BaseAttention, and CausalSelfAttention classes here\n",
    "# They should include necessary layers and methods as previously discussed\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, *, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            vocab_size=input_vocab_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# Example setup for training\n",
    "num_layers = 4  # Number of transformer layers\n",
    "num_heads = 8  # Number of attention heads\n",
    "dff = 2048  # Dimensionality of the feed-forward layer\n",
    "dropout_rate = 0.1  # Dropout rate\n",
    "\n",
    "# Initialize the Transformer model\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size,\n",
    "    dropout_rate=dropout_rate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9fe2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41063903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13eff9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5411/5411 [==============================] - 523s 96ms/step - loss: 4.5585 - masked_accuracy: 0.2241 - val_loss: 4.3594 - val_masked_accuracy: 0.2404\n"
     ]
    }
   ],
   "source": [
    "transformer.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
    "history = transformer.fit(train_batches, epochs=1, validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fdc9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Python\"\n",
    "x = vectorize_layer(sentence)\n",
    "x = tf.expand_dims(x, axis=0)\n",
    "prediction = transformer(x)\n",
    "predicted_id = tf.argmax(prediction, axis=-1)\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary=vocabulary, mask_token=\"\", oov_token=\"[UNK]\", invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "391dffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_concat = tf.experimental.numpy.append(x, predicted_id[0], axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcccf04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        vocabulary,\n",
    "        transformer,\n",
    "        max_new_tokens,\n",
    "        temperature=0.0,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        sentence = self.tokenizer(sentence)\n",
    "        sentence = tf.expand_dims(sentence, axis=0)\n",
    "        encoder_input = sentence\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "\n",
    "        print(f\"Generating {self.max_new_tokens} tokens\")\n",
    "        for i in tf.range(self.max_new_tokens):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer(encoder_input, training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "            if self.temperature == 0.0:\n",
    "                # greedy sampling, output always the same\n",
    "                predicted_id = tf.argmax(predictions, axis=-1)\n",
    "            else:\n",
    "                predictions = predictions / self.temperature\n",
    "                predicted_id = tf.random.categorical(predictions[0], num_samples=1)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "            output_array = output_array.write(i + 1, predicted_id[0])\n",
    "            encoder_input = tf.experimental.numpy.append(encoder_input, predicted_id[0])\n",
    "            encoder_input = tf.expand_dims(encoder_input, axis=0)\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # The output shape is `(1, tokens)`.\n",
    "        id_to_word = tf.keras.layers.StringLookup(\n",
    "            vocabulary=self.vocabulary, mask_token=\"\", oov_token=\"[UNK]\", invert=True\n",
    "        )\n",
    "\n",
    "        print(f\"Using temperature of {self.temperature}\")\n",
    "        text = id_to_word(output)\n",
    "        tokens = output\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer(output[:, :-1], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19d1ec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50 tokens\n",
      "Using temperature of 0.92\n",
      "Input:         : Machine learning\n",
      "Generation     : [[b'' b'[UNK]' b'with' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]'\n",
      "  b'[UNK]' b'[UNK]' b'to' b'[UNK]' b'security' b'image' b'courtesy'\n",
      "  b'[UNK]' b'if' b'you' b'just' b'want' b'to' b'read' b'it' b'from'\n",
      "  b'the' b'[UNK]' b'of' b'the' b'internet' b'that' b'is' b'possible'\n",
      "  b'when' b'you' b'need' b'to' b'download' b'the' b'version' b'of'\n",
      "  b'case' b'you' b'have' b'an' b'online' b'media' b'accounts' b'[UNK]'\n",
      "  b'[UNK]' b'in' b'person']]\n"
     ]
    }
   ],
   "source": [
    "def print_generation(sentence, generated_text):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Generation\":15s}: {generated_text}')\n",
    "\n",
    "max_new_tokens = 50\n",
    "temperature = 0.92\n",
    "\n",
    "generator = Generator(vectorize_layer, vocabulary, transformer, max_new_tokens, temperature)\n",
    "sentence = \"Machine learning\"\n",
    "generated_text, generated_tokens, attention_weights = generator(sentence)\n",
    "print_generation(sentence, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f20a6c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50 tokens\n",
      "Using temperature of 0.92\n",
      "Input:         : Python\n",
      "Generation     : [[b'' b'from' b'[UNK]' b'in' b'every' b'free' b'live' b'online' b'[UNK]'\n",
      "  b'a' b'[UNK]' b'of' b'income' b'learn' b'how' b'to' b'use' b'linkedin'\n",
      "  b'machine' b'learning' b'machine' b'learning' b'companies' b'how'\n",
      "  b'did' b'you' b'pursue' b'a' b'successful' b'career' b'in' b'your'\n",
      "  b'organization' b'and' b'how' b'could' b'you' b'navigate' b'the'\n",
      "  b'music' b'sector' b'you' b'ask' b'whether' b'you' b'put' b'a' b'live'\n",
      "  b'job' b'and' b'not']]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Python\"\n",
    "\n",
    "def plot_attention_weights(sentence, generated_tokens, attention_heads):\n",
    "    in_tokens = vectorize_layer([sentence])\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    for h, head in enumerate(attention_heads):\n",
    "        ax = fig.add_subplot(2, 4, h + 1)\n",
    "\n",
    "        plot_attention_weights(in_tokens, generated_tokens, head)\n",
    "\n",
    "        ax.set_xlabel(f\"Head {h+1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generated_text, generated_tokens, attention_weights = generator(sentence)\n",
    "print_generation(sentence, generated_text)\n",
    "# plot_attention_weights(sentence, generated_tokens, attention_weights[0])\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50035f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
